{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa2s7CaEyL0iIZB79Nv6vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smrutipunto/DS-for-argri/blob/main/DS_Agri_4_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opIsAvR1EaCz"
      },
      "outputs": [],
      "source": [
        "# Practical 4 Analyzing Satellite Images for Crop Health Monitoring: Use satellite imagery to assess crop health and identify areas needing attention. Tools: QGIS, Python, OpenCV.\n",
        "#pracrical 4\n",
        "# Analyzing Satellite Images for Crop Health Monitoring: Use\n",
        "satellite imagery to assess crop health and identify areas needing\n",
        "attention. Tools: QGIS, Python, OpenCV\n",
        "var r=ee.ImageCollection(\"projects/neon-prod\u0002earthengine/assets/HSI_REFL/001\");\n",
        "print('Neon Directional Reflectance\n",
        "Images:',r.aggregate_array('system:index'));\n",
        "\n",
        "var startdate=ee.Date('2017-01-01');\n",
        "var enddate=startdate.advance(1,'year');\n",
        "var r_2017=r.filterDate(startdate,enddate);\n",
        "var r_grsm_2017=r_2017.filter(ee.Filter.eq('NEON_SITE',\n",
        "'GRSM')).mosaic();\n",
        "var rgbvis={\n",
        " min:340,\n",
        " max:2150,\n",
        " bands: ['B053','B035','B019'],\n",
        " gamma:2\n",
        "};\n",
        "Map.addLayer(r_grsm_2017,rgbvis,'Reflectance RGB Image');\n",
        "var nirband=r_grsm_2017.select('B170');\n",
        "var redband=r_grsm_2017.select('B050');\n",
        "var\n",
        "ndvi=nirband.subtract(redband).divide(nirband.add(redband)).rename('\n",
        "NDVI');\n",
        "Map.addLayer(ndvi,{min:-\n",
        "1,max:1,palette:['blue','white','green']},'NDVI')\n",
        "var hv=ndvi.gt(0.8).selfMask();\n",
        "var sv=ndvi.lt(0.6).selfMask();\n",
        "Map.addLayer(hv,{min:-1,max:1,palette:['green']},'High Vegetation')\n",
        "Map.addLayer(sv,{min:-1,max:1,palette:['red']},'Stressed\n",
        "Vegetation')\n",
        "Map.setCenter(-83.5, 35.65, 14);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 5\n",
        "# Land Use Classification Using Remote Sensing Data: Classify land\n",
        "use patterns using remote sensing data and mac\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Load the EuroSAT dataset.\n",
        "# The dataset contains 27,000+ labeled satellite images (64x64\n",
        "pixels, 3 bands)\n",
        "# It is widely used as a benchmark for land cover classification.\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        " 'eurosat',\n",
        " split=['train[:80%]', 'train[80%:]'],\n",
        " as_supervised=True, # returns tuple (image, label)\n",
        " with_info=True\n",
        ")\n",
        "# Preprocessing function to normalize image pixels to [0,1]\n",
        "def preprocess(image, label):\n",
        " image = tf.cast(image, tf.float32) / 255.0\n",
        " return image, label\n",
        "# Set up dataset pipelines: apply preprocessing, shuffle and batch.\n",
        "BATCH_SIZE = 32\n",
        "ds_train =\n",
        "ds_train.map(preprocess).shuffle(1000).batch(BATCH_SIZE).prefetch(tf\n",
        ".data.AUTOTUNE)\n",
        "ds_test =\n",
        "ds_test.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "# Retrieve input shape and number of classes from ds_info.\n",
        "input_shape = ds_info.features['image'].shape # e.g., (64, 64, 3)\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "# Retrieve the class names from the dataset info.\n",
        "class_names = ds_info.features['label'].names\n",
        "print(\"Class names:\", class_names)\n",
        "# Build a Convolutional Neural Network (CNN) using Keras.\n",
        "model = tf.keras.Sequential([\n",
        " tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
        "input_shape=input_shape),\n",
        " tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "27\n",
        " tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        " tf.keras.layers.MaxPooling2D((2, 2)),\n",
        " tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(128, activation='relu'),\n",
        " tf.keras.layers.Dropout(0.5),\n",
        " tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "# Compile the model with an optimizer, loss function, and accuracy\n",
        "metric.\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "# Train the model.\n",
        "EPOCHS = 10\n",
        "history = model.fit(ds_train,\n",
        " epochs=EPOCHS,\n",
        " validation_data=ds_test)\n",
        "# Evaluate the model on the test dataset.\n",
        "test_loss, test_acc = model.evaluate(ds_test)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "# -------------------------------\n",
        "# Visualize Predictions on Test Images\n",
        "# -------------------------------\n",
        "# Take one batch from the test dataset\n",
        "for images, labels in ds_test.take(1):\n",
        " predictions = model.predict(images)\n",
        " pred_labels = np.argmax(predictions, axis=1)\n",
        " true_labels = labels.numpy()\n",
        "# Set up a grid for visualization.\n",
        "num_images = 9 # number of images to display\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_images):\n",
        " ax = plt.subplot(3, 3, i + 1)\n",
        " plt.imshow(images[i].numpy())\n",
        " pred_class = class_names[pred_labels[i]]\n",
        " true_class = class_names[true_labels[i]]\n",
        " plt.title(f\"True: {true_class}\\nPred: {pred_class}\")\n",
        " plt.axis(\"off\")\n",
        "28\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "Class names: ['AnnualCrop', 'Forest', 'HerbaceousVegetation',\n",
        "'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential',\n",
        "'River', 'SeaLake']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "rwKcNpvSI417",
        "outputId": "0ba59ee2-3043-4e20-bef4-2302448d0c86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (<ipython-input-1-fdcd6c5f792d>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fdcd6c5f792d>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    pixels, 3 bands)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical 6  Soil Moisture Prediction Using Sensor Data: Predict soil moisture levels using data from soil sensors and environmental factors.\n",
        "# Soil Moisture Prediction Using Sensor Data: Predict soil moisture\n",
        "levels using data from soil sensors and environmental factors.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Dropout\n",
        "from keras.regularizers import L1L2\n",
        "from keras.callbacks import EarlyStopping\n",
        "df = pd.read_csv('soil_data.csv')\n",
        "# Data overview\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "# Descriptive statistics\n",
        "print(\"\\nDescriptive statistics of the dataset:\")\n",
        "print(df.describe())\n",
        "# Determine unique latitude-longitude pairs\n",
        "unique_locations = df[['latitude',\n",
        "'longitude']].drop_duplicates().reset_index(drop=True)\n",
        "print(f\"There are {unique_locations.shape[0]} unique locations in\n",
        "the dataset.\")\n",
        "m = folium.Map(location=[51.1657, 10.4515], zoom_start=5,\n",
        "tiles=\"OpenStreetMap\", width=800, height=800)\n",
        "# Add unique locations to the map\n",
        "for idx, row in unique_locations.iterrows():\n",
        " folium.CircleMarker(\n",
        " location=(row['latitude'], row['longitude']),\n",
        " radius=2,\n",
        " color='blue',\n",
        " fill=True,\n",
        " fill_color='blue',\n",
        " fill_opacity=0.6\n",
        " ).add_to(m)\n",
        " # Display the map directly\n",
        "m\n",
        "      # Create a unique identifier for each location\n",
        "df['location_id'] = df.groupby(['latitude', 'longitude']).ngroup()\n",
        "33\n",
        "# Determine the start and end of the time series\n",
        "start_date = df['time'].min()\n",
        "end_date = df['time'].max()\n",
        "print(f\"The time series starts on {start_date} and ends on\n",
        "{end_date}.\")\n",
        "# Randomly select five unique location IDs\n",
        "random_locations = np.random.choice(df['location_id'].unique(), 5,\n",
        "replace=False)\n",
        "# Plot time series data for the selected locations\n",
        "plt.figure(figsize=(10, 5))\n",
        "for location in random_locations:\n",
        " subset = df[df['location_id'] == location]\n",
        " lat = subset['latitude'].iloc[0]\n",
        " lon = subset['longitude'].iloc[0]\n",
        " plt.plot(pd.to_datetime(subset['time']), subset['sm_tgt'],\n",
        "label=f\"Location {location} (Lat: {lat:.2f}, Lon: {lon:.2f})\")\n",
        "plt.title(\"Soil Moisture Time Series for Randomly Selected\n",
        "Locations\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Soil Moisture Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Set style and context to make the plot look fancy\n",
        "34\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "# Create the distribution plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.histplot(df['sm_tgt'], kde=True, bins=30, color='skyblue',\n",
        "edgecolor='black', linewidth=1.2)\n",
        "# Mark percentiles on the plot\n",
        "percentiles = [5, 25, 50, 75, 95]\n",
        "for percentile in percentiles:\n",
        " value = np.percentile(df['sm_tgt'], percentile)\n",
        " plt.axvline(value, color='red', linestyle='dashed', linewidth=1)\n",
        " plt.text(value, 5, f'{percentile}%', color='red', rotation=90,\n",
        "verticalalignment='bottom')\n",
        "# Set the title and labels\n",
        "plt.title(\"Distribution of Soil Moisture for the Year 2013\",\n",
        "fontsize=18)\n",
        "plt.xlabel(\"Soil Moisture Value\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Randomly sample 2000 data points from the dataframe\n",
        "sample_df = df.sample(n=2000, random_state=42)\n",
        "# Set the aesthetic style of the plots\n",
        "35\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "# Initialize a figure with three subplots side by side\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n",
        "# List of soil components\n",
        "components = ['clay_content', 'sand_content', 'silt_content']\n",
        "# Plot scatter plots for each component\n",
        "for i, component in enumerate(components):\n",
        " sns.regplot(x=component, y='sm_tgt', data=sample_df, ax=axes[i],\n",
        "color='skyblue', scatter_kws={'s':10}, line_kws={'color':'red'})\n",
        "\n",
        "# Calculate correlation and annotate the plot with its value\n",
        " correlation = sample_df[component].corr(sample_df['sm_tgt'])\n",
        " axes[i].set_title(f\"Correlation between\n",
        "{component.split('_')[0].capitalize()} and Soil Moisture:\n",
        "{correlation:.2f}\", fontsize=14)\n",
        "# Adjust layout for better display\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Convert the 'time' column to datetime format\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "# Separate the data for June\n",
        "june_data = df[df['time'].dt.month == 6]\n",
        "# Remove June data from the main dataframe\n",
        "df = df[df['time'].dt.month != 6]\n",
        "june_data.head()\n",
        "# Features to normalize\n",
        "features_to_normalize = ['latitude', 'longitude', 'clay_content',\n",
        "'sand_content', 'silt_content', 'sm_aux', 'location_id','sm_tgt']\n",
        "# Initialize scaler\n",
        "scaler = MinMaxScaler()\n",
        "# Apply normalization to the dataframe (both training and June data)\n",
        "df[features_to_normalize] =\n",
        "scaler.fit_transform(df[features_to_normalize])\n",
        "june_data[features_to_normalize] =\n",
        "scaler.transform(june_data[features_to_normalize])\n",
        "# Reorder the columns to ensure 'sm_tgt' is the last column\n",
        "ordered_columns = ['time', 'latitude', 'longitude', 'clay_content',\n",
        "'sand_content', 'silt_content', 'sm_aux', 'location_id', 'sm_tgt']\n",
        "df = df[ordered_columns]\n",
        "june_data = june_data[ordered_columns]\n",
        "df.head()\n",
        "june_data.head()\n",
        "# Calculate the number of data points for each location in june_data\n",
        "data_points_per_location = june_data.groupby('location_id').size()\n",
        "# Identify locations with less than 15 days of data\n",
        "locations_to_exclude =\n",
        "data_points_per_location[data_points_per_location < 15].index\n",
        "# Filter out these locations from june_data\n",
        "june_data =\n",
        "june_data[~june_data['location_id'].isin(locations_to_exclude)]\n",
        "def create_sequences(data, seq_length):\n",
        " \"\"\"\n",
        " Create input-output sequence pairs from the provided dataframe.\n",
        " \"\"\"\n",
        " X, Y = [], []\n",
        " for location, group in data.groupby('location_id'):\n",
        " # Extract relevant features from the group\n",
        " features =\n",
        "group[features_to_normalize].drop(columns='sm_tgt').values\n",
        " targets = group['sm_tgt'].values\n",
        "\n",
        " # Create sequences\n",
        " for i in range(len(features) - seq_length):\n",
        " X.append(features[i:i+seq_length])\n",
        " Y.append(targets[i+seq_length])\n",
        "\n",
        " return X, Y\n",
        "# Define the sequence length (30 days)\n",
        "seq_length = 15\n",
        "# Create sequences for training data\n",
        "X, Y = create_sequences(df, seq_length)\n",
        "# Convert lists to numpy arrays for easier handling later\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X.shape, Y.shape\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, BatchNormalization, Dropout, Dense\n",
        "from keras.regularizers import l2\n",
        "model = Sequential()\n",
        "model.add(LSTM(75, activation='relu', input_shape=(X.shape[1],\n",
        "X.shape[2]), kernel_regularizer=l2(0.01),\n",
        "recurrent_regularizer=l2(0.01), return_sequences=True))\n",
        "model.add(LSTM(75, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "# Dense output layer\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3,\n",
        "verbose=1, restore_best_weights=True)\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X, Y, epochs=10, batch_size=128,\n",
        "validation_split=0.2, verbose=1, callbacks=[early_stop])\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "Code:\n",
        "_june, Y_june_true = create_sequences(june_data, seq_length)\n",
        "X_june = np.array(X_june);\n",
        "Y_june_true = np.array(Y_june_true);\n",
        "model.compile(optimizer='adam', loss='mse');\n",
        "X_june.shape,Y_june_true.shape\n",
        "predictions = model.predict(X_june,batch_size=128)\n",
        "def reverse_scaling(data, scaler):\n",
        " \"\"\"\n",
        " Reverse the scaling effect on data.\n",
        " \"\"\"\n",
        " # Convert data to a 2D array if it's 1D\n",
        " if len(data.shape) == 1:\n",
        " data = data.reshape(-1, 1)\n",
        "\n",
        " dummy = np.zeros((len(data), len(features_to_normalize)))\n",
        "\n",
        " # Set the last column of the dummy array to your data\n",
        " dummy[:, -1] = data.ravel()\n",
        "\n",
        " # Use inverse_transform to reverse the scaling\n",
        " unscaled = scaler.inverse_transform(dummy)\n",
        "\n",
        " # Return the last column (our actual unscaled data)\n",
        " return unscaled[:, -1]\n",
        "# Reverse scaling for Y_june_true and predictions\n",
        "Y_june_true_unscaled = reverse_scaling(Y_june_true, scaler)\n",
        "predictions_unscaled = reverse_scaling(predictions, scaler)\n",
        "# Ensure both arrays are 1-dimensional\n",
        "Y_june_true_unscaled = Y_june_true_unscaled.ravel()\n",
        "predictions_unscaled = predictions_unscaled.ravel()\n",
        "# Create a DataFrame for plotting\n",
        "df_plot = pd.DataFrame({\n",
        " 'Actual Values': Y_june_true_unscaled,\n",
        " 'Predicted Values': predictions_unscaled\n",
        "})\n",
        "# Set Seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "# Create a line plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=df_plot)\n",
        "plt.title('Comparison of Actual vs Predicted Soil Moisture for\n",
        "June')\n",
        "plt.ylabel('Soil Moisture')\n",
        "plt.xlabel('Days in June')\n",
        "40\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(Y_june_true_unscaled - predictions_unscaled))\n",
        "print(f\"Mean Absolute Error for June Predictions: {mae}\")"
      ],
      "metadata": {
        "id": "JK_65sn7J8Xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}